{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DevilNReality/Qwen2_Powered_Chatbot/blob/main/Code%20File%20/%20Qwen2_Powered_ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYWyuNXGfBOJ"
      },
      "source": [
        " # **ðŸ”§ Section 1: Setup & Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q7PF5E9eiOJ"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VlK48csfFkZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Define the pre-trained model name\n",
        "model_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
        "\n",
        "# Load the tokenizer for the specified model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the pre-trained causal language model\n",
        "# device_map=\"auto\" automatically handles model placement on available devices (like GPU)\n",
        "# trust_remote_code=True is needed for some custom models\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Set the padding token to be the same as the end-of-sequence token\n",
        "# This is important for batch processing in training and inference\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9yNlQqmfRnW"
      },
      "source": [
        "# **ðŸ§© Section 2: System Prompt & History Initialization**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTJd6PWEoKaw"
      },
      "outputs": [],
      "source": [
        "# Initialize the chat history with a system prompt and an initial assistant message\n",
        "# This sets the persona and starts the conversation\n",
        "chat_history = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful, friendly AI assistant. Respond conversationally and clearly.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Hi there! I'm your friendly AI assistant. How can I help you today?\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ULW84SsoNBO"
      },
      "source": [
        "# **ðŸ§© Section 3: Response Generation Logic**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVyJE_VGfLTL"
      },
      "outputs": [],
      "source": [
        "def generate_response(user_input, model, tokenizer, max_length=512):\n",
        "    # Add the user's input to the chat history\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # Build the prompt string from the chat history\n",
        "    # Each turn is formatted with special tokens like <|system|>, <|user|>, <|assistant|>\n",
        "    prompt = \"\"\n",
        "    for turn in chat_history:\n",
        "        if turn[\"role\"] == \"system\":\n",
        "            prompt += f\"<|system|>\\n{turn['content']}\\n\"\n",
        "        elif turn[\"role\"] == \"user\":\n",
        "            prompt += f\"<|user|>\\n{turn['content']}\\n\"\n",
        "        elif turn[\"role\"] == \"assistant\":\n",
        "            prompt += f\"<|assistant|>\\n{turn['content']}\\n\"\n",
        "    # Add the assistant token at the end to prompt the model to generate the next assistant response\n",
        "    prompt += \"<|assistant|>\\n\"\n",
        "\n",
        "    # Tokenize the prompt and move it to the model's device\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate a response from the model\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,  # Set the maximum length of the generated response\n",
        "        pad_token_id=tokenizer.eos_token_id, # Use the end-of-sequence token for padding\n",
        "        do_sample=True,         # Enable sampling for more diverse responses\n",
        "        top_k=50,               # Consider the top 50 most likely tokens\n",
        "        top_p=0.95,             # Use nucleus sampling (consider tokens that sum up to 95% probability)\n",
        "        temperature=0.7         # Control the randomness of the output (lower means less random)\n",
        "    )\n",
        "\n",
        "    # Decode the generated tokens back into text\n",
        "    # Skip special tokens and split the output to get only the last assistant message\n",
        "    reply = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"<|assistant|>\\n\")[-1].strip()\n",
        "\n",
        "    # Add the generated assistant reply to the chat history\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": reply})\n",
        "\n",
        "    # Return only the content of the last assistant message\n",
        "    return chat_history[-1][\"content\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEyl2Yj4gzIk"
      },
      "source": [
        "# **ðŸ’¬ Section 4: Gradio UI Integration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVPkjSfVgAgO"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create a Gradio chatbot component initialized with the chat history\n",
        "chatbot = gr.Chatbot(value=chat_history, type='messages')\n",
        "\n",
        "# Create a Gradio ChatInterface\n",
        "gr.ChatInterface(\n",
        "    fn=lambda user_input, history: generate_response(user_input, model, tokenizer), # The function to call when the user sends a message\n",
        "    title=\"Chat with AI\", # Title of the chat interface\n",
        "    description=\"A conversational assistant powered by Qwen2-0.5B-Instruct\", # Description of the chat interface\n",
        "    chatbot=chatbot # Link the ChatInterface to the chatbot component\n",
        ").launch(debug = True) # Launch the Gradio interface, debug=True provides detailed logs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ§© Section 5: Instruction Tuning Dataset Prep**"
      ],
      "metadata": {
        "id": "E_shTMEQlJBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the training data as a dictionary\n",
        "data = {\n",
        "    \"input\": [\n",
        "        \"Summarize the concept of photosynthesis.\",\n",
        "        \"Translate 'Good morning' to French.\",\n",
        "        \"List three uses of artificial intelligence.\"\n",
        "    ],\n",
        "    \"output\": [\n",
        "        \"Photosynthesis is the process by which green plants convert sunlight, carbon dioxide, and water into energy, releasing oxygen as a byproduct.\",\n",
        "        \"Bonjour\",\n",
        "        \"1. Personalized recommendations in apps and websites\\n2. Autonomous vehicles and robotics\\n3. Fraud detection in banking and finance\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a pandas DataFrame from the data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file named \"train_data.csv\"\n",
        "# index=False prevents writing the DataFrame index as a column\n",
        "df.to_csv(\"train_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "8aJVf_iUxuBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "# \"csv\" specifies the format, data_files points to the file, and split=\"train\" loads the training split\n",
        "dataset = load_dataset(\"csv\", data_files=\"train_data.csv\", split=\"train\")\n",
        "\n",
        "# Define a function to format each example in the dataset\n",
        "# It takes an example (a row from the dataset) and formats it into a prompt string\n",
        "# using the special tokens expected by the model\n",
        "def format_example(example):\n",
        "    prompt = f\"<|user|>\\n{example['input']}\\n<|assistant|>\\n{example['output']}\"\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "# Apply the formatting function to each example in the dataset\n",
        "# .map() applies the function to each element and returns a new dataset\n",
        "formatted_dataset = dataset.map(format_example)"
      ],
      "metadata": {
        "id": "NQAuj0XjmI7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer again for processing the fine-tuning dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
        "\n",
        "# Set the padding token to the end-of-sequence token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define a function to tokenize the formatted examples\n",
        "# This function takes an example (which now has a 'text' field)\n",
        "# and converts the text into token IDs that the model understands\n",
        "# padding=\"max_length\" ensures all sequences have the same length by adding padding tokens\n",
        "# truncation=True cuts off sequences longer than the model's maximum input length\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Apply the tokenization function to the formatted dataset\n",
        "# batched=True processes examples in batches, which is more efficient\n",
        "tokenized_dataset = formatted_dataset.map(tokenize, batched=True)"
      ],
      "metadata": {
        "id": "60xI6_rEmK-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ§© Section 6: Fine-Tuning Loop**"
      ],
      "metadata": {
        "id": "xmuvyG8wm96M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "\n",
        "# Define the model name again\n",
        "model_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
        "\n",
        "# Load the pre-trained model for fine-tuning\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Load the tokenizer again\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set the padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "ts811ZWumQJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training arguments for the Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen2-instruct-tuned\",  # Directory to save model checkpoints and outputs\n",
        "    per_device_train_batch_size=2, # Batch size per GPU/device during training\n",
        "    num_train_epochs=3,           # Number of training epochs\n",
        "    logging_steps=10,             # Log training progress every X steps\n",
        "    save_steps=50,                # Save a model checkpoint every X steps\n",
        "    save_total_limit=2,           # Limit the total number of saved checkpoints\n",
        "    weight_decay=0.01,            # Apply weight decay to prevent overfitting\n",
        "    fp16=True,                    # Use mixed precision training (faster on supported hardware)\n",
        "    report_to=\"none\"              # Do not report training metrics to external services\n",
        ")"
      ],
      "metadata": {
        "id": "tViqdgXXnBfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Trainer\n",
        "# The Trainer is a class that simplifies the training loop for ðŸ¤— Transformers models\n",
        "trainer = Trainer(\n",
        "    model=model,               # The model to train\n",
        "    args=training_args,        # The training arguments defined above\n",
        "    train_dataset=tokenized_dataset, # The tokenized training dataset\n",
        "    tokenizer=tokenizer        # The tokenizer used for the model and data\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "R3tkKyZ4nD17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ§© Section 7: Evaluation**"
      ],
      "metadata": {
        "id": "PuvEBysQoHZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Define the path to the fine-tuned model\n",
        "model_path = \"./qwen2-instruct-tuned\"\n",
        "\n",
        "# Load the fine-tuned model from the specified path\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Load the tokenizer associated with the fine-tuned model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Set the padding token for the loaded tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "rDCMUrKbnNvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to test the fine-tuned model with a given prompt\n",
        "def test_prompt(prompt, max_length=512):\n",
        "    # Format the user prompt with the required tokens\n",
        "    formatted = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n\"\n",
        "\n",
        "    # Tokenize the formatted prompt and move it to the model's device\n",
        "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate a response using the fine-tuned model\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length, # Set the maximum length of the generated response\n",
        "        pad_token_id=tokenizer.eos_token_id, # Use the end-of-sequence token for padding\n",
        "        do_sample=True,         # Enable sampling\n",
        "        top_k=50,               # Consider top_k tokens\n",
        "        top_p=0.95,             # Use top_p sampling\n",
        "        temperature=0.7         # Set the temperature\n",
        "    )\n",
        "\n",
        "    # Decode the generated output and extract the assistant's reply\n",
        "    reply = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"<|assistant|>\\n\")[-1].strip()\n",
        "\n",
        "    # Return the generated reply\n",
        "    return reply"
      ],
      "metadata": {
        "id": "gW94tBLhoMkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the fine-tuned model with example prompts\n",
        "print(test_prompt(\"Translate 'Good night' to French.\"))\n",
        "print(test_prompt(\"Give me three uses of AI.\"))\n",
        "print(test_prompt(\"Summarize the concept of gravity.\"))"
      ],
      "metadata": {
        "id": "hrS1uSB7oOpT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxVaAgj0spVXvttCGpg3gB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}